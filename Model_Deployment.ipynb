{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKsEYRKMIXz6"
      },
      "source": [
        "# Hunting Exoplanets In Space - Deploying A Prediction Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdmZPtYSn7kA"
      },
      "source": [
        "The prediction model, through the training dataset, will learn the properties of a star that has a planet and also the properties of a star which does not have a planet. Once the model has learnt the required properties, it will look for these properties in the test dataset and according to the properties it sees, it will predict whether a star has a planet or not.\n",
        "\n",
        "We will deploy the **Random Forest Classifier** model (a machine learning algorithm). Machine learning is a branch of artificial intelligence in which a machine learns through data the different features on its own without being programmed by a computer programmer.\n",
        "\n",
        "\n",
        "<img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/lesson-14/ml-vs-ai.png' width=700>\n",
        "\n",
        "In this case, the machine will learn to recognise the flux values of stars having a planet on its own. When a new dataset containing only the flux values of a star is shown to the machine, it will tell the star having a planet and not having a planet.\n",
        "\n",
        "There are many machine learning models or algorithms to do this kind of prediction. One of them is **Random Forest Classifier**. It is used to classify outcomes into classes (or labels) based on some features. For e.g., an animal that makes a 'Meow! Meow!' sound is classified (or labelled) as a cat, an animal that makes a 'Woof! Woof!' sound is classified as a dog, an animal which makes a hissing sound is classified as a snake etc.\n",
        "\n",
        "We will use the Random Forest Classifier model to classify whether a star has a planet or not. The stars which have at least one planet are labelled as `2` while the stars not having a planet are labelled as `1`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq9mAtEg9cpZ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwzryU6Zie33"
      },
      "source": [
        "#### Loading The Training Dataset\n",
        "\n",
        "(Use the Links to the (or download) datasets are too large to upload here)\n",
        "\n",
        "Dataset links:\n",
        "\n",
        "1. Train dataset\n",
        "   \n",
        "   https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/kepler-exoplanets-dataset/exoTrain.csv\n",
        "\n",
        "2. Test dataset\n",
        "   \n",
        "   https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/kepler-exoplanets-dataset/exoTest.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zjScdqmQstFg"
      },
      "outputs": [],
      "source": [
        "# Loading both the training and test datasets.\n",
        "import pandas as pd\n",
        "\n",
        "exo_train_df = pd.read_csv('exoTrain.csv')\n",
        "exo_test_df = pd.read_csv('exoTest.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest Classifier - Working\n",
        "\n",
        "A Random Forest is a collection (a.k.a. ensemble) of many decision trees. A decision tree is a flow chart which separates data based on some condition. If a condition is true, you move on a path otherwise, you move on to another path.\n",
        "\n",
        "\n",
        "For e.g., in case of finding a star having a planet, you can construct the following decision tree:\n",
        "\n",
        "<img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/lesson-14/decision-tree.png' width=600>\n",
        "\n",
        "You could ask a question whether there is decrease in the flux values of a star. If the answer is no, then it clearly means the star does not have a planet. However, if the answer is yes, then you could ask another question to check whether the decrease is periodic or not. Again, if the answer is no, then the star does not have a planet. Otherwise, it has a planet.\n",
        "\n",
        "This is one of the examples of a decision tree. Based on a problem, the decision tree could get more and more complex.\n",
        "\n",
        "A collection of `N` number of trees is a random forest wherein each tree gives some predicted value (in this case either class `1` or class `2`).\n",
        "\n",
        "<img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/lesson-14/rfc-image.jpg' width=800>\n",
        "\n",
        "The final predicted value is the majority class, i.e, the class that is predicted by the most number of decision trees in a random forest.\n",
        "\n",
        "For the time being, just consider the Random Forest Classifier as some kind of a black-box which classifies data into different classes (in this case, either class `1` or class `2`) by learning the properties of every class through a training dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob_UHtcNaE-s"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### In Our Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l7BTsS7LNLE"
      },
      "source": [
        "There are `565` stars which are classified as `1` and `5` stars classified as `2` which means only `5` stars have a planet. Interestingly, if our prediction model mindlessly classifies every star as `1`, then it is a very accurate model. Why?\n",
        "\n",
        "Because the accuracy of a model is calculated as **a percentage of the correct predictions out of the total number of predictions**. In this case, the percentage of the correct predictions is\n",
        "\n",
        " $\\frac{565\\times100}{570} = 99.122$ %\n",
        "\n",
        "Thus, without actually deploying a proper prediction model, we can predict the stars having a planet with 99% accuracy.\n",
        "\n",
        "This is **WRONG**! This is where we need to be careful. Because we have very imbalanced data. The ultimate goal of the Kepler space telescope is to detect exoplanets in outer space. Hence, a machine learning model, based on some data should also **correctly** detect stars having planets. This means a prediction model will be considered useful if it correctly detects almost all the stars having a planet.\n",
        "\n",
        "So, the prediction model which always labels every star as `1` is useless. Because it must detect almost all the stars having a planet.\n",
        "\n",
        "Now, we are going to deploy the Random Forest Classifier model so that it can detect all the five (or at least three) stars having a planet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zr_egypdy1R"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Understanding Random Forest Classifier**\n",
        "\n",
        "#### Introduction  \n",
        "The **Random Forest Classifier** is an ensemble learning algorithm that improves prediction accuracy by combining multiple decision trees. It is widely used for classification and regression tasks due to its high accuracy, robustness, and ability to handle large datasets.  \n",
        "\n",
        "#### How It Works  \n",
        "1. **Bootstrap Sampling (Bagging)**: The dataset is randomly sampled with replacement to create multiple subsets.  \n",
        "2. **Decision Tree Training**: Each subset is used to train an independent decision tree.  \n",
        "3. **Random Feature Selection**: Instead of using all features, each tree selects a random subset of features at each split.  \n",
        "4. **Majority Voting (Classification)**: For a classification task, each tree predicts a class, and the most common class (majority vote) is chosen as the final prediction.  \n",
        "5. **Averaging (Regression)**: For regression tasks, the final prediction is the average of all tree predictions.  \n",
        "\n",
        "#### Key Concepts  \n",
        "\n",
        "#### 1. **Ensemble Learning**  \n",
        "Random Forest is an **ensemble method**, meaning it combines multiple weak learners (decision trees) to form a stronger model, reducing the risk of overfitting.  \n",
        "\n",
        "#### 2. **Feature Randomness**  \n",
        "Unlike a single decision tree that considers all features, Random Forest introduces randomness by selecting only a subset of features at each node split. This reduces correlation between trees, improving generalization.  \n",
        "\n",
        "#### 3. **Bias-Variance Tradeoff**  \n",
        "- A single decision tree has **low bias** but **high variance** (overfits the training data).  \n",
        "- Random Forest balances bias and variance by averaging multiple trees, leading to lower variance while maintaining reasonable bias.  \n",
        "\n",
        "#### 4. **Handling Imbalanced Data**  \n",
        "In datasets like exoplanet detection, where one class is significantly larger than the other, Random Forest can be adjusted using:  \n",
        "- **Class Weights**: Assigning higher weights to the minority class.  \n",
        "- **Downsampling or Upsampling**: Adjusting the dataset to balance class distribution.  \n",
        "\n",
        "#### Hyperparameters in Random Forest  \n",
        "\n",
        "| Hyperparameter | Description |\n",
        "|---------------|-------------|\n",
        "| `n_estimators` | Number of decision trees in the forest |\n",
        "| `max_depth` | Maximum depth of each tree (prevents overfitting) |\n",
        "| `min_samples_split` | Minimum samples required to split a node |\n",
        "| `min_samples_leaf` | Minimum samples required in a leaf node |\n",
        "| `max_features` | Number of features considered for best split |\n",
        "| `bootstrap` | Whether to sample with replacement |\n",
        "\n",
        "#### Advantages of Random Forest  \n",
        "✅ **Handles missing values** and outliers well  \n",
        "✅ **Prevents overfitting** with multiple trees  \n",
        "✅ **Works well with large datasets**  \n",
        "✅ **Reduces variance** compared to a single decision tree  \n",
        "✅ **Provides feature importance ranking**  \n",
        "\n",
        "#### Disadvantages of Random Forest  \n",
        "❌ **Computationally expensive** for very large datasets  \n",
        "❌ **Less interpretable** compared to a single decision tree  \n",
        "❌ **Not ideal for real-time applications** due to high inference time  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Importing `RandomForestClassifier`\n",
        "\n",
        "We need to import a module called `RandomForestClassifier` from a package called `sklearn.ensemble`. The `sklearn` (or **scikit-learn**) is a collection of many machine learning modules. Almost every machine learning algorithm can be directly applied without a knowledge of math using the **scikit-learn** library. It is kind of a plug-and-play device.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Target & Feature Variables Separation\n",
        "\n",
        "The `RandomForestClassifier` module has a function called `fit()` which takes two inputs. The first input is the collection of feature variables.\n",
        "\n",
        "*The features are those variables which describe the features or properties of an entity.* In this case, the `FLUX.1` to `FLUX.3197` are feature variables. Hence, the values stored in these columns are the features of a star in exoplanets dataset.\n",
        "\n",
        "The second input is the target variable.\n",
        "\n",
        "*The variable which needs to be predicted is called a target variable.* In this case, the `LABEL` is the target variable because the prediction model needs to predict which star belongs to which class in the test dataset. Hence, the values stored in the `LABEL` column are the target values.\n",
        "\n",
        "So, we need to extract the target variable and the feature variables separately from the training dataset.\n",
        "\n",
        "Let's store the feature variables in the `x_train` variable and the target variable in the `y_train` variable. We will separate the features using the `iloc[]` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FLUX.1</th>\n",
              "      <th>FLUX.2</th>\n",
              "      <th>FLUX.3</th>\n",
              "      <th>FLUX.4</th>\n",
              "      <th>FLUX.5</th>\n",
              "      <th>FLUX.6</th>\n",
              "      <th>FLUX.7</th>\n",
              "      <th>FLUX.8</th>\n",
              "      <th>FLUX.9</th>\n",
              "      <th>FLUX.10</th>\n",
              "      <th>...</th>\n",
              "      <th>FLUX.3188</th>\n",
              "      <th>FLUX.3189</th>\n",
              "      <th>FLUX.3190</th>\n",
              "      <th>FLUX.3191</th>\n",
              "      <th>FLUX.3192</th>\n",
              "      <th>FLUX.3193</th>\n",
              "      <th>FLUX.3194</th>\n",
              "      <th>FLUX.3195</th>\n",
              "      <th>FLUX.3196</th>\n",
              "      <th>FLUX.3197</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>93.85</td>\n",
              "      <td>83.81</td>\n",
              "      <td>20.10</td>\n",
              "      <td>-26.98</td>\n",
              "      <td>-39.56</td>\n",
              "      <td>-124.71</td>\n",
              "      <td>-135.18</td>\n",
              "      <td>-96.27</td>\n",
              "      <td>-79.89</td>\n",
              "      <td>-160.17</td>\n",
              "      <td>...</td>\n",
              "      <td>-78.07</td>\n",
              "      <td>-102.15</td>\n",
              "      <td>-102.15</td>\n",
              "      <td>25.13</td>\n",
              "      <td>48.57</td>\n",
              "      <td>92.54</td>\n",
              "      <td>39.32</td>\n",
              "      <td>61.42</td>\n",
              "      <td>5.08</td>\n",
              "      <td>-39.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-38.88</td>\n",
              "      <td>-33.83</td>\n",
              "      <td>-58.54</td>\n",
              "      <td>-40.09</td>\n",
              "      <td>-79.31</td>\n",
              "      <td>-72.81</td>\n",
              "      <td>-86.55</td>\n",
              "      <td>-85.33</td>\n",
              "      <td>-83.97</td>\n",
              "      <td>-73.38</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.28</td>\n",
              "      <td>-32.21</td>\n",
              "      <td>-32.21</td>\n",
              "      <td>-24.89</td>\n",
              "      <td>-4.86</td>\n",
              "      <td>0.76</td>\n",
              "      <td>-11.70</td>\n",
              "      <td>6.46</td>\n",
              "      <td>16.00</td>\n",
              "      <td>19.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>532.64</td>\n",
              "      <td>535.92</td>\n",
              "      <td>513.73</td>\n",
              "      <td>496.92</td>\n",
              "      <td>456.45</td>\n",
              "      <td>466.00</td>\n",
              "      <td>464.50</td>\n",
              "      <td>486.39</td>\n",
              "      <td>436.56</td>\n",
              "      <td>484.39</td>\n",
              "      <td>...</td>\n",
              "      <td>-71.69</td>\n",
              "      <td>13.31</td>\n",
              "      <td>13.31</td>\n",
              "      <td>-29.89</td>\n",
              "      <td>-20.88</td>\n",
              "      <td>5.06</td>\n",
              "      <td>-11.80</td>\n",
              "      <td>-28.91</td>\n",
              "      <td>-70.02</td>\n",
              "      <td>-96.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>326.52</td>\n",
              "      <td>347.39</td>\n",
              "      <td>302.35</td>\n",
              "      <td>298.13</td>\n",
              "      <td>317.74</td>\n",
              "      <td>312.70</td>\n",
              "      <td>322.33</td>\n",
              "      <td>311.31</td>\n",
              "      <td>312.42</td>\n",
              "      <td>323.33</td>\n",
              "      <td>...</td>\n",
              "      <td>5.71</td>\n",
              "      <td>-3.73</td>\n",
              "      <td>-3.73</td>\n",
              "      <td>30.05</td>\n",
              "      <td>20.03</td>\n",
              "      <td>-12.67</td>\n",
              "      <td>-8.77</td>\n",
              "      <td>-17.31</td>\n",
              "      <td>-17.35</td>\n",
              "      <td>13.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1107.21</td>\n",
              "      <td>-1112.59</td>\n",
              "      <td>-1118.95</td>\n",
              "      <td>-1095.10</td>\n",
              "      <td>-1057.55</td>\n",
              "      <td>-1034.48</td>\n",
              "      <td>-998.34</td>\n",
              "      <td>-1022.71</td>\n",
              "      <td>-989.57</td>\n",
              "      <td>-970.88</td>\n",
              "      <td>...</td>\n",
              "      <td>-594.37</td>\n",
              "      <td>-401.66</td>\n",
              "      <td>-401.66</td>\n",
              "      <td>-357.24</td>\n",
              "      <td>-443.76</td>\n",
              "      <td>-438.54</td>\n",
              "      <td>-399.71</td>\n",
              "      <td>-384.65</td>\n",
              "      <td>-411.79</td>\n",
              "      <td>-510.54</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 3197 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    FLUX.1   FLUX.2   FLUX.3   FLUX.4   FLUX.5   FLUX.6  FLUX.7   FLUX.8  \\\n",
              "0    93.85    83.81    20.10   -26.98   -39.56  -124.71 -135.18   -96.27   \n",
              "1   -38.88   -33.83   -58.54   -40.09   -79.31   -72.81  -86.55   -85.33   \n",
              "2   532.64   535.92   513.73   496.92   456.45   466.00  464.50   486.39   \n",
              "3   326.52   347.39   302.35   298.13   317.74   312.70  322.33   311.31   \n",
              "4 -1107.21 -1112.59 -1118.95 -1095.10 -1057.55 -1034.48 -998.34 -1022.71   \n",
              "\n",
              "   FLUX.9  FLUX.10  ...  FLUX.3188  FLUX.3189  FLUX.3190  FLUX.3191  \\\n",
              "0  -79.89  -160.17  ...     -78.07    -102.15    -102.15      25.13   \n",
              "1  -83.97   -73.38  ...      -3.28     -32.21     -32.21     -24.89   \n",
              "2  436.56   484.39  ...     -71.69      13.31      13.31     -29.89   \n",
              "3  312.42   323.33  ...       5.71      -3.73      -3.73      30.05   \n",
              "4 -989.57  -970.88  ...    -594.37    -401.66    -401.66    -357.24   \n",
              "\n",
              "   FLUX.3192  FLUX.3193  FLUX.3194  FLUX.3195  FLUX.3196  FLUX.3197  \n",
              "0      48.57      92.54      39.32      61.42       5.08     -39.54  \n",
              "1      -4.86       0.76     -11.70       6.46      16.00      19.93  \n",
              "2     -20.88       5.06     -11.80     -28.91     -70.02     -96.67  \n",
              "3      20.03     -12.67      -8.77     -17.31     -17.35      13.98  \n",
              "4    -443.76    -438.54    -399.71    -384.65    -411.79    -510.54  \n",
              "\n",
              "[5 rows x 3197 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Taking out the features\n",
        "x_train = exo_train_df.iloc[:,1:]\n",
        "x_train[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       2\n",
              "1       2\n",
              "2       2\n",
              "3       2\n",
              "4       2\n",
              "       ..\n",
              "5082    1\n",
              "5083    1\n",
              "5084    1\n",
              "5085    1\n",
              "5086    1\n",
              "Name: LABEL, Length: 5087, dtype: int64"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Taking out the target\n",
        "y_train = exo_train_df.iloc[:,0]\n",
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8FljiKddvz9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest Hyperparameters\n",
        "\n",
        "Random Forest is an ensemble learning method that constructs multiple decision trees to improve predictive accuracy and control overfitting. Understanding its hyperparameters is crucial for effective model tuning. Here's a breakdown of key hyperparameters:\n",
        "\n",
        "##### n_estimators\n",
        "\n",
        "- **Definition**: Specifies the number of trees in the forest.\n",
        "\n",
        "- **Impact**: Increasing the number of trees generally enhances model performance by reducing variance. However, after a certain point, adding more trees yields diminishing returns and increases computational cost.\n",
        "\n",
        "##### max_depth\n",
        "\n",
        "- **Definition**: Sets the maximum depth of each decision tree.\n",
        "\n",
        "- **Impact**: Deeper trees can capture more complex patterns but might overfit the data. Limiting the depth helps in balancing bias and variance.\n",
        "\n",
        "##### min_samples_split\n",
        "\n",
        "- **Definition**: The minimum number of samples required to split an internal node.\n",
        "\n",
        "- **Impact**: Higher values prevent the model from learning overly specific patterns (overfitting) by ensuring that splits occur only when a sufficient number of samples are present.\n",
        "\n",
        "##### min_samples_leaf\n",
        "\n",
        "- **Definition**: The minimum number of samples required to be at a leaf node.\n",
        "\n",
        "- **Impact**: Larger values result in leaves that contain more data, promoting generalization by preventing the model from learning noise in the training data.\n",
        "\n",
        "##### max_features\n",
        "\n",
        "- **Definition**: Determines the number of features to consider when looking for the best split.\n",
        "\n",
        "- **Impact**: Using fewer features can make the model more diverse but might miss important information. Common settings include 'sqrt' (square root of the total number of features) and 'log2' (base-2 logarithm of the total number of features).\n",
        "\n",
        "##### bootstrap\n",
        "\n",
        "- **Definition**: Decides whether to use all data points when building each tree or to sample data points with replacement.\n",
        "\n",
        "- **Impact**: Sampling with replacement (bootstrap) introduces randomness that can improve model robustness. If set to False, the whole dataset is used to build each tree.\n",
        "\n",
        "Understanding and tuning these hyperparameters can significantly influence the performance of a Random Forest model, allowing for better control over the balance between bias and variance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAU1jEBVkHG1",
        "outputId": "24842761-1750-4b75-911c-48b8abd93339"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best Parameters: {'n_estimators': np.int64(150), 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 20, 'bootstrap': True}\n",
            "Optimized Training Accuracy: 0.9972\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define the hyperparameter distribution\n",
        "param_dist = {\n",
        "    'n_estimators': np.arange(50, 201, 50),  # Number of trees (50, 100, 150, 200)\n",
        "    'max_depth': [None, 10, 15, 20],  # Tree depth\n",
        "    'min_samples_split': [2, 5, 10],  # Minimum samples to split a node\n",
        "    'min_samples_leaf': [1, 2, 4],  # Minimum samples per leaf\n",
        "    'max_features': ['sqrt', 'log2'],  # Number of features to consider per split\n",
        "    'bootstrap': [True, False]  # Bootstrap sampling\n",
        "}\n",
        "\n",
        "# Initialize Random Forest\n",
        "rf_clf = RandomForestClassifier(n_jobs=-1)\n",
        "\n",
        "# Perform Randomized Search\n",
        "random_search = RandomizedSearchCV(\n",
        "    rf_clf, param_distributions=param_dist, \n",
        "    n_iter=20, cv=5, scoring='accuracy', \n",
        "    n_jobs=-1, verbose=2, random_state=42\n",
        ")\n",
        "\n",
        "# Train on dataset\n",
        "random_search.fit(x_train, y_train)\n",
        "\n",
        "# Get best parameters\n",
        "best_params = random_search.best_params_\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "\n",
        "# Train the final model with the best parameters\n",
        "best_rf_clf = RandomForestClassifier(**best_params, n_jobs=-1)\n",
        "best_rf_clf.fit(x_train, y_train)\n",
        "\n",
        "# Evaluate performance\n",
        "train_accuracy = best_rf_clf.score(x_train, y_train)\n",
        "print(f\"Optimized Training Accuracy: {train_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f3OVTo4ioQk"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz8BPprpipOq"
      },
      "source": [
        "#### The `predict()` Function\n",
        "\n",
        "Now, let's make predictions on the test dataset by calling the `predict()` function with the features variables of the test dataset as an input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF_lb5Fcp8zd",
        "outputId": "ac1f2eb9-c92d-42f3-a8fa-030a679d2c0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "prediction = best_rf_clf.predict(exo_test_df.iloc[:,1:])\n",
        "prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pKzHPiEjCSI"
      },
      "source": [
        "The predict function returns a NumPy array of the predicted values. You can verify it using the `type()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOz8o4fJp-_W",
        "outputId": "4d140c87-c05e-4d08-a2ce-e9d2bbe74c75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "print(type(prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhdBmNx8jWQv"
      },
      "source": [
        "The actual target values are stored in a Pandas series. So, for the sake of consistency, let's convert the NumPy array of the predicted values into a Pandas series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF3skQa0qBeO",
        "outputId": "7ed8c713-5dbe-4b85-a120-a1d2113ee452"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      1\n",
              "1      1\n",
              "2      1\n",
              "3      1\n",
              "4      1\n",
              "      ..\n",
              "565    1\n",
              "566    1\n",
              "567    1\n",
              "568    1\n",
              "569    1\n",
              "Length: 570, dtype: int64"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd_series = pd.Series([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
        "pd_series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQwrexkrjrdE"
      },
      "source": [
        "Now, let's count the number of stars classified as `1` and `2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qk7UnroAqTUT",
        "outputId": "df6f5ddc-c3aa-4eb7-dacd-eb45232160f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    570\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd_series.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfKiAcaMj5xF"
      },
      "source": [
        "As you can see, we did not get the expected results. The model should have classified all the stars having a planet as `2`. Ideally, the Random Forest Classifier model should have classified `565` values as `1` and the remaining `5` values as `2`.\n",
        "\n",
        "In this case, even though the accuracy of a prediction model is high but according to the problem statement, it is not giving the desired result. Hence, **accuracy alone is not the metric to test the efficacy of a prediction model.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Understanding XGBoost Classifier**\n",
        "\n",
        "#### **1. What is XGBoost?**\n",
        "XGBoost (Extreme Gradient Boosting) is a machine learning algorithm designed for structured/tabular data. It is an **optimized version of Gradient Boosting**, designed to be **faster, more efficient, and accurate**.\n",
        "\n",
        "#### **2. Why Use XGBoost?**\n",
        "- **Speed**: Uses parallel and distributed computing.\n",
        "- **Accuracy**: Handles missing data and overfitting well.\n",
        "- **Flexibility**: Works for regression, classification, ranking, etc.\n",
        "- **Regularization**: Prevents overfitting using L1 (Lasso) & L2 (Ridge) penalties.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. How Does XGBoost Work?**\n",
        "\n",
        "##### **Step 1: Understanding Boosting**\n",
        "Boosting is an ensemble method that combines multiple weak models (typically decision trees) to create a strong model. It works in an iterative manner:\n",
        "1. Train a weak model (Decision Tree) on the dataset.\n",
        "2. Identify the mistakes (errors) made by the model.\n",
        "3. Train the next model to correct these errors.\n",
        "4. Repeat the process to reduce errors progressively.\n",
        "\n",
        "XGBoost applies **Gradient Boosting**, where each new model learns from the previous model’s mistakes using a gradient descent approach.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Mathematical Working of XGBoost**\n",
        "\n",
        "##### **Step 2: Objective Function**\n",
        "XGBoost optimizes the following function:\n",
        "\n",
        "\\[ Obj = Loss + \\Omega \\]\n",
        "\n",
        "Where:\n",
        "- **Loss**: Measures how well the model fits the training data.\n",
        "- **\\(\\Omega\\)**: Regularization term to prevent overfitting.\n",
        "\n",
        "##### **Step 3: Loss Function (Gradient Descent)**\n",
        "XGBoost uses a second-order Taylor expansion of the loss function:\n",
        "\n",
        "\\[ L(\\theta) \\approx \\sum_{i=1}^{n} [g_i \\theta + \\frac{1}{2} h_i \\theta^2] \\]\n",
        "\n",
        "Where:\n",
        "- **\\(g_i\\)** = First derivative (gradient) of the loss function.\n",
        "- **\\(h_i\\)** = Second derivative (hessian) of the loss function.\n",
        "\n",
        "This helps in efficient learning and better optimization.\n",
        "\n",
        "##### **Step 4: Tree Construction**\n",
        "Each new decision tree is built to minimize the loss function.\n",
        "- **Gain** is calculated for each split:\n",
        "\n",
        "\\[ Gain = \\frac{1}{2} \\left[ \\frac{(\\sum g)^2}{\\sum h + \\lambda} \\right] \\]\n",
        "\n",
        "Where **\\(\\lambda\\)** is a regularization parameter.\n",
        "\n",
        "- The **best split** is chosen based on maximum gain.\n",
        "- The tree grows until it reaches a stopping condition (e.g., max depth, min gain, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Hyperparameters in XGBoost**\n",
        "Some key hyperparameters that control the model:\n",
        "- `n_estimators`: Number of trees.\n",
        "- `max_depth`: Depth of each tree.\n",
        "- `learning_rate`: Step size for updating weights.\n",
        "- `subsample`: Fraction of data used per tree.\n",
        "- `colsample_bytree`: Features used per tree.\n",
        "- `lambda` & `alpha`: L2 and L1 regularization terms.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Example Code for XGBoost Classifier**\n",
        "```python\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Model\n",
        "model = xgb.XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, use_label_encoder=False)\n",
        "\n",
        "# Train Model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **Conclusion**\n",
        "XGBoost is one of the most powerful classification algorithms for structured data. Its ability to handle missing values, outliers, and large datasets makes it a go-to choice in machine learning competitions and real-world applications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7CYlKb-R3VM",
        "outputId": "cb2b24c4-07fd-4883-926b-3bcd077ce55c"
      },
      "outputs": [],
      "source": [
        "import xgboost as xg\n",
        "\n",
        "md = xg.XGBClassifier()\n",
        "md.fit(x_train,y_train-1)\n",
        "y_pred = md.predict(exo_test_df.iloc[:,1:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred+1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Even After using the XGboost Classifier we got the same results.\n",
        "Let's first evaluate the model and then try process the data. Then we can try again "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3WclLnZkFfz"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The Confusion Matrix\n",
        "Let's quickly first create a confusion matrix and then will try to understand it.\n",
        "\n",
        "To create a confusion matrix, first import `confusion_matrix` module from the `sklearn.metrics` library. This library contains all the parameters to evaluate a machine learning model. In addition to the `confusion_matrix` module, let's also import the `classification_report` module. We will use them later to evaluate our module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test = exo_test_df.iloc[:,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[565,   0],\n",
              "       [  5,   0]])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "confusion_matrix(y_test, y_pred+1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have got our confusion matrix, let's try to understand this concept. So, after you deploy the classification model, there are 4 possible outcomes. They are:\n",
        "\n",
        "1. Class `1` values predicted as class `1`. In this case,\n",
        "\n",
        "    - the class `1` values are stars **NOT** having a planet, and\n",
        "\n",
        "    - the class `2` values are stars having a planet\n",
        "\n",
        "2. Class `1` values predicted as class `2`.\n",
        "\n",
        "3. Class `2` values predicted as class `1`.\n",
        "\n",
        "4. Class `2` values predicted as class `2`.\n",
        "\n",
        "These 4 possibilities can be reported in a table which is called a confusion matrix.\n",
        "\n",
        "||Predicted Class `1` (`y_predicted`)|Predicted Class `2` (`y_predicted`)|\n",
        "|-|-|-|\n",
        "|Actual Class `1` (`y_test`)|||\n",
        "|Actual Class `2` (`y_test`)|||\n",
        "\n",
        "where\n",
        "\n",
        "- `y_test` contains the actual class `1` & actual class `2` values\n",
        "\n",
        "- `y_predicted` contains the predicted class `1` & predicted class `2` values\n",
        "\n",
        "In this table,\n",
        "\n",
        "- the values **predicted as class `1` and actually belonging to class `1`** are reported in the first row and first column.\n",
        "\n",
        "||Predicted Class `1` (`y_predicted`)|Predicted Class `2` (`y_predicted`)|\n",
        "|-|-|-|\n",
        "|Actual Class `1` (`y_test`)|565||\n",
        "|Actual Class `2` (`y_test`)|||\n",
        "\n",
        "- the values **predicted as class `1` but actually belonging to class `2`** are reported in the second row and first column.\n",
        "\n",
        "||Predicted Class `1` (`y_predicted`)|Predicted Class `2` (`y_predicted`)|\n",
        "|-|-|-|\n",
        "|Actual Class `1` (`y_test`)|565||\n",
        "|Actual Class `2` (`y_test`)|5||\n",
        "\n",
        "- the values **predicted as class `2` and actually belonging to class `2`** are reported in the second row and second column.\n",
        "\n",
        "||Predicted Class `1` (`y_predicted`)|Predicted Class `2` (`y_predicted`)|\n",
        "|-|-|-|\n",
        "|Actual Class `1` (`y_test`)|565||\n",
        "|Actual Class `2` (`y_test`)|5|0|\n",
        "\n",
        "- the values **predicted as class `2` but actually belonging to class `1`** are reported in the first row and second column.\n",
        "\n",
        "||Predicted Class `1` (`y_predicted`)|Predicted Class `2` (`y_predicted`)|\n",
        "|-|-|-|\n",
        "|Actual Class `1` (`y_test`)|565|0|\n",
        "|Actual Class `2` (`y_test`)|5|0|\n",
        "\n",
        "In this case, the class `1` values refer to the stars not having a planet whereas class `2` values refer to the stars having a planet.\n",
        "\n",
        "**Positive Outcome**\n",
        "\n",
        "Detecting a star having a planet is our desired outcome. In technical terms, the desired outcome is called a *positive outcome*. So, here the positive outcome is the prediction of the stars having a planet, i.e., prediction of the class `2` values. Likewise, finding a star which does not have any planet is a *negative outcome*. So, here the negative outcome is the prediction of the class `1` values.\n",
        "\n",
        "Thus,\n",
        "\n",
        "- `565` values are **True Negative (TN)** values because they are **truly** predicted as class `1` values.\n",
        "\n",
        "||Predicted Class `1` (`y_predicted`)|Predicted Class `2` (`y_predicted`)|\n",
        "|-|-|-|\n",
        "|Actual Class `1` (`y_test`)|565 (TN)||\n",
        "|Actual Class `2` (`y_test`)|||\n",
        "\n",
        "\n",
        "- `5` values are **False Negative (FN)** values because they are **falsely** predicted as class `1` values. They should have been predicted as class `2` values.\n",
        "\n",
        "||Predicted Class `1` (`y_predicted`)|Predicted Class `2` (`y_predicted`)|\n",
        "|-|-|-|\n",
        "|Actual Class `1` (`y_test`)|565 (TN)||\n",
        "|Actual Class `2` (`y_test`)|5 (FN)||\n",
        "\n",
        "\n",
        "- `0` values are **True Positive (TP)** values because they are **truly** predicted as class `2` values.\n",
        "\n",
        "||Predicted Class `1` (`y_predicted`)|Predicted Class `2` (`y_predicted`)|\n",
        "|-|-|-|\n",
        "|Actual Class `1` (`y_test`)|565 (TN)||\n",
        "|Actual Class `2` (`y_test`)|5 (FN)|0 (TP)|\n",
        "\n",
        "\n",
        "- `0` values are **False Positive (FP)** values because they are **falsely** predicted as class `2` values. They should have been predicted as class `1` values.\n",
        "\n",
        "||Predicted Class `1` (`y_predicted`)|Predicted Class `2` (`y_predicted`)|\n",
        "|-|-|-|\n",
        "|Actual Class `1` (`y_test`)|565 (TN)|0 (FP)|\n",
        "|Actual Class `2` (`y_test`)|5 (FN)|0 (TP)|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Precision And Recall\n",
        "\n",
        "A good prediction model provides a very large number of true positive (TP) values and a very low number of true negative values.\n",
        "\n",
        "Now, based on the TP and FP values, we define a parameter called **precision**. It is the ratio of the TP values to the sum of TP and FP values, i.e., \"Of all the instances the model said are positive, how many are actually positive?\"\n",
        "\n",
        "$$\\text{precision} = \\frac{\\text{TP}}{\\text{TP + FP}}$$\n",
        "\n",
        "Currently, the model has given `0` TP values and `0` FP values. Therefore, the precision value is undefined because\n",
        "\n",
        "$$\\text{precision} = \\frac{0}{0 + 0} = \\text{undefined}$$\n",
        "\n",
        "*In mathematics, the division by 0 is undefined (or not defined).*\n",
        "\n",
        "Based on the TP and FN values, we define another parameter called **recall**. It is the ratio of the TP values to the sum of TP and FN values, i.e,  \"Of all the actual positive instances, how many did the model correctly predict?\"\n",
        "\n",
        "$$\\text{recall} = \\frac{\\text{TP}}{\\text{TP + FN}}$$\n",
        "\n",
        "Currently, the model gives `0` TP and `5` FN values. Hence, the recall value is 0 because\n",
        "\n",
        "$$\\text{recall} = \\frac{0}{0+5} = \\text{0}$$\n",
        "\n",
        "Imagine if the prediction model labels every star as `2`, i.e, every star has a planet. Then, the number of TP values will be the maximum, i.e., `5` but the number of FP values will also be maximum, i.e., `565`. In such a case, the precision value would be\n",
        "\n",
        "$$\\text{precision} = \\frac{5}{5+565} = \\frac{5}{570} = 0.008$$\n",
        "\n",
        "which is very very low.\n",
        "\n",
        "Also, the model will give `0` FN values. Then, the recall value would be\n",
        "\n",
        "$$\\text{recall} = \\frac{5}{5 + 0} = 1$$\n",
        "\n",
        "\n",
        "So, even though the recall value would be equal to 1, the precision value would be close to 0. Hence, this would be a bad prediction model.\n",
        "\n",
        "\n",
        "Evidently, there is a trade-off. If the recall value is high, then the precision value will be low and vice-versa. Hence, we need to find an optimum point where both, the precision and the recall values are acceptable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The `f1-score`\n",
        "\n",
        "To find an optimum point where both, the precision and recall values, are high, we calculate another parameter called **f1-score**. It is a harmonic mean of the precision and recall values, i.e.,\n",
        "\n",
        "$$\\text{f1-score} = 2 \\left( \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} \\right)$$\n",
        "\n",
        "So, based on the current predictions, the f1-scores value is undefined because both the precision and recall values are also undefined.\n",
        "\n",
        "$$\\text{f1-score} = 2 \\left( \\frac{\\text{undefined} \\times 0}{\\text{undefined} + 0} \\right) =  \\text{undefined}$$\n",
        "\n",
        "You can also get these values by calling a function called `classification_report()`. It takes two inputs: the actual target values and the predicted target values, i.e., `y_test` and `y_predicted`.\n",
        "\n",
        "**Note:** You may get the following warning message after executing the code in the code cell below.\n",
        "\n",
        "```\n",
        "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      1.00      1.00       565\n",
            "           2       0.00      0.00      0.00         5\n",
            "\n",
            "    accuracy                           0.99       570\n",
            "   macro avg       0.50      0.50      0.50       570\n",
            "weighted avg       0.98      0.99      0.99       570\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, y_pred+1))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
